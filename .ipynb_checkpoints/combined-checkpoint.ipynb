{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#only need once\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1212: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "import functools\n",
    "import os\n",
    "import re\n",
    "from gensim.summarization import keywords\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import networkx as nx\n",
    "import codecs\n",
    "\n",
    "#global variable\n",
    "STOPWORDS = set(stopwords.words(\"english\"))\n",
    "STEMMER = SnowballStemmer(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = './Disruptive Innovation Text/*.txt' \n",
    "files = glob.glob(path)\n",
    "# iterate over the list getting each file \n",
    "articles_list = []\n",
    "#inside for loop we remove the global redundants\n",
    "for fle in files:\n",
    "    with codecs.open(fle, \"r\",encoding='utf-8', errors='ignore') as f:\n",
    "        article_content = ''\n",
    "        for line in f:\n",
    "            article_content += line\n",
    "        articles_list.append(article_content)\n",
    "articles_dict = {'article_text': articles_list}\n",
    "df = pd.DataFrame.from_dict(articles_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\r\\nInnovating in an Evolving \\r\\nWorld: \\r\\nSlow Change \\r\\nwith \\r\\nLong-Term Impact \\r\\n\\r\\nHumans are hard-wired to respond to instantaneous \\r\\nchange: Our fight or flight \\r\\nresponse evolved to make snap decisions based on \\r\\nimmediate danger. It is not in \\r\\nour nature to identify and react to challenges that arise slowly, even those with great \\r\\nlong-term \\r\\nimpact on \\r\\nour lives. \\r\\n\\r\\n\\r\\nBusinesses face a \\r\\nsimilar problem when dealing with \\r\\nslow change \\r\\nin \\r\\ntheir \\r\\nindustries. Large-scale societal, economic, and technological trends that emerge \\r\\ngradually and continuously over time can be \\r\\nall too easily \\r\\noverlooked. \\r\\nThat is \\r\\nbecause \\r\\nslow change \\r\\noften occurs outside \\r\\nof an organizations line-of-sight and \\r\\ncan \\r\\narise from the merger of many disparate \\r\\ndevelopments, each of which may be \\r\\nlost in the day-to-day \\r\\nnoise. \\r\\n\\r\\n\\r\\nEven the \\r\\nmost forward-thinking companies can \\r\\nmiss \\r\\nslow change \\r\\noccurring around \\r\\nthem: \\r\\nIn 2007, Microsoft CEO Steve Ballmer said, There \\r\\nis \\r\\nno chance the iPhone \\r\\nis going to \\r\\nget significant market share.1 By neglecting \\r\\nsteady, continuous \\r\\nadvancements \\r\\nin \\r\\ncomputing power and mobile technology, Microsoft missed the \\r\\nsmartphone phenomenon \\r\\nand \\r\\nsaw its market capitalization \\r\\nfall from $642 \\r\\nbillion in \\r\\n\\r\\n2,3\\r\\n\\r\\n2000 to $270 billion in \\r\\n2014.\\r\\n\\r\\nIn a time when the \\r\\naverage \\r\\ncompany lifespan on \\r\\nthe S&P 500 has dropped from 61 \\r\\nyears to \\r\\n18 years, spotting \\r\\nslow change is \\r\\ncritical for long-term stability and growth.4 \\r\\nAt Citi, we track \\r\\nslow change \\r\\nby asking which \\r\\ntrends are \\r\\naffecting our businesses \\r\\nand \\r\\nclients in ways that are \\r\\nboth accelerating \\r\\nand \\r\\nirreversible. The results are \\r\\nevident in exponentially-improved systems \\r\\nthat reduce friction or shift society in \\r\\nsuch a way that returning to \\r\\nthe \\r\\nold practices seems inconceivable. \\r\\n\\r\\nBy seeking \\r\\nout such patterns in our work at Citi, we have identified three types of \\r\\nchange that often \\r\\ncause \\r\\nthe \\r\\nbiggest impact: \\r\\n(1) \\r\\nBehavioral \\r\\nchanges, \\r\\n(2) \\r\\nTechnological changes, \\r\\nand \\r\\n(3) Industry or sector changes. \\r\\n\\r\\n\\r\\nThe most profound transformations incorporate at least one \\r\\nof these \\r\\nchanges, but \\r\\nmany \\r\\noften \\r\\ninclude \\r\\nmore. Such is \\r\\nthe \\r\\ncase with three trends \\r\\nwe are currently \\r\\nexploring: (1) changing \\r\\nsocial \\r\\nstructures, \\r\\n(2) the \\r\\nchanging nature of transactions, \\r\\nand (3) the \\r\\nchanging nature of industries. '"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#global redudant removal for each row (which is a string) 1 by 1\n",
    "#replace raw txt from the df with the new txt (removing global redundancy)\n",
    "remove_global_redundancies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function to remove stopwords\n",
    "def remove_stopwords(sen):\n",
    "    sen_new = \" \".join([i for i in sen if i not in stop_words])\n",
    "    return sen_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract word vectors\n",
    "word_embeddings = {}\n",
    "f = open('./gloves.6B/glove.6B.100d.txt', encoding='utf-8')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    word_embeddings[word] = coefs\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################ONLY DO ONCE FOR THE FIRST 12 ARTICLES TO GET THE FIRST FREQ TABLE!##############\n",
    "\n",
    "#get frequency table from text.\n",
    "def get_freq_table(txt):\n",
    "    words = word_tokenize(txt) #list of strings where each string is a word.\n",
    "    freq_table = dict()\n",
    "    \n",
    "    for word in words:\n",
    "        word = STEMMER.stem(word)\n",
    "        if word in STOPWORDS:\n",
    "            continue\n",
    "        if word in freq_table:\n",
    "            freq_table[word] += 1\n",
    "        else:\n",
    "            freq_table[word] = 1\n",
    "       \n",
    "    return freq_table\n",
    "\n",
    "#create giant string which contains all articles (which had global redundancies removed prior)\n",
    "overall_string = \"\"\n",
    "for s in df['article_text']:\n",
    "    overall_string = s + \" \"\n",
    "\n",
    "#global word frequency table\n",
    "FREQ_TABLE = get_freq_table(overall_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load freq table from pkl file\n",
    "def load_freq_table(pkl_dir): #wishful thinking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#update freq table pkl file\n",
    "#pre-con: pkl_dir is directory of pkl file, txt is a text file with global redundancies removed, whereby\n",
    "#         we update the freq table counts by the words in txt\n",
    "def update_freq_table(pkl_dir, txt):\n",
    "    #read pkl_dir wishful thinking\n",
    "    freq_table = load_freq_table(pkl_dir)\n",
    "    #list of words\n",
    "    words_in_txt = word_tokenize(txt)\n",
    "    \n",
    "    for word in words_in_txt:\n",
    "        stemmed_word = STEMMER.stem(word)\n",
    "        if stemmed_word in STOPWORDS:\n",
    "            continue\n",
    "        if word in freq_table:\n",
    "            freq_table[stemmed_word] += 1\n",
    "        else:\n",
    "            freq_table[stemmed_word] = 1\n",
    "       \n",
    "    #write_freq_table wishful thinking\n",
    "    #write_freq_table(pkl_dir, words_in_txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pre-cond: txt is a text with global redundancies removed. pkl_dir is the directory of the freq_table pickle.\n",
    "\n",
    "def summarize_alg(txt, pkl_dir):\n",
    "    sentences = nltk.sent_tokenize(txt) #list of strings where each string is a sentence\n",
    "    \n",
    "    #-------------clean sentences (non-global redundants)---------------------\n",
    "    \n",
    "    # remove punctuations, numbers and special characters\n",
    "    clean_sentences = pd.Series(sentences).str.replace(\"[^a-zA-Z -]\", \"\").str.replace(\" +\", \" \")\n",
    "    # make alphabets lowercase\n",
    "    clean_sentences = [s.lower() for s in clean_sentences]\n",
    "    # remove stopwords from the sentences\n",
    "    clean_sentences = [remove_stopwords(r.split()) for r in clean_sentences]\n",
    "    \n",
    "    #-------------------------1st algo------------------------\n",
    "    sentence_vectors = []\n",
    "    for i in clean_sentences:\n",
    "        if len(i) != 0:\n",
    "            v = sum([word_embeddings.get(w, np.zeros((100,))) for w in i.split()])/(len(i.split())+0.001)\n",
    "        else:\n",
    "            v = np.zeros((100,))\n",
    "        sentence_vectors.append(v)\n",
    "        \n",
    "    # similarity matrix initialization\n",
    "    sim_mat = np.zeros([len(sentences), len(sentences)])\n",
    "    \n",
    "    # updating matrix coeffs\n",
    "    for i in range(len(sentences)):\n",
    "        for j in range(len(sentences)):\n",
    "            if i != j:\n",
    "                sim_mat[i][j] = cosine_similarity(sentence_vectors[i].reshape(1,100),\n",
    "                                                  sentence_vectors[j].reshape(1,100))[0,0]\n",
    "    \n",
    "    # graphs\n",
    "    nx_graph = nx.from_numpy_matrix(sim_mat)\n",
    "    first_algo_scores = nx.pagerank_numpy(nx_graph, alpha=0.85, personalization=None, weight='weight', dangling=None)\n",
    "    \n",
    "    #normalize across first_algo_scores such that every score runs from [0,1].\n",
    "    first_algo_scores = {k: (np.arctan(v)/np.pi + 1/2) for k, v in first_algo_scores.items()}\n",
    "    \n",
    "    #--------------------2nd algo-----------------------------------------------\n",
    "    #load freq table\n",
    "    freq_table = load_freq_table(pkl_dir) #wishful thinking\n",
    "    \n",
    "    second_algo_scores = dict()\n",
    "    \n",
    "    for i in range(len(clean_sentences)):\n",
    "        sentence = clean_sentences[i]\n",
    "        words_in_sentence = word_tokenize(sentence)\n",
    "        for word in words_in_sentence:\n",
    "            if STEMMER.stem(word) in freq_table: #we only consider words which appear in freq_table.\n",
    "                score_of_word = freq_table[STEMMER.stem(word)]\n",
    "                if i in second_algo_scores:\n",
    "                    second_algo_scores[i] += score_of_word\n",
    "                else:\n",
    "                    second_algo_scores[i] = score_of_word\n",
    "        \n",
    "        #divide by length of sentence\n",
    "        second_algo_scores[i] /= len(sentence)\n",
    "        \n",
    "    #normalize across second_algo_scores such that every score runs from [0,1].\n",
    "    max_score = max(second_algo_scores.values())\n",
    "    second_algo_scores = {k: v/max_score for k, v in second_algo_scores.items()}\n",
    "    \n",
    "    #----------------------combine scores----------------------------\n",
    "    combined_scores = {i: first_algo_scores[i]*second_algo_scores[i] for i in range(len(sentences))}\n",
    "    \n",
    "    #----------------------extract the sentences we need-----------------------------------\n",
    "    #list of tuples (index, score) sorted by score\n",
    "    scores_sorted = list(sorted(combined_scores.items(), lambda x:-x[1]))\n",
    "    \n",
    "    extracted_sentences = list()\n",
    "    #first article no trailing \" \". so initialize to -1\n",
    "    character_count = -1\n",
    "    for (index, score) in scores_sorted:\n",
    "        if character_count + len(sentences[index]) > 500:\n",
    "            break\n",
    "        else:\n",
    "            extracted_sentences.append((index, sentences[index]))\n",
    "            character_count += len(sentences[index])+1 #+1 to take into account the trailing \" \"\n",
    "\n",
    "    #-----------return the sentences in the order they appeared as a single string--------\n",
    "    final_text = functools.reduce(lambda x,y : x+\" \"+y, map(lambda x: x[1], sorted(extracted_sentences)))\n",
    "    \n",
    "    return final_text\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
