{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1212: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "import functools\n",
    "import os\n",
    "import re\n",
    "from gensim.summarization import keywords\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import networkx as nx\n",
    "import codecs\n",
    "import pickle\n",
    "import time\n",
    "\n",
    "#timer\n",
    "start = time.time()\n",
    "\n",
    "#global variable\n",
    "STOPWORDS = set(stopwords.words(\"english\"))\n",
    "STEMMER = SnowballStemmer(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_txt(txtfile):\n",
    "    with open(txtfile, 'r') as myfile:\n",
    "        text=myfile.read()\n",
    "    myfile.close()\n",
    "    return text\n",
    "\n",
    "#pre-con: txt is a raw text with \\n\n",
    "#post-con: returns text with global redundant words removed\n",
    "def remove_global_redundancies(txt):\n",
    "    txt = re.sub(\"\\\\n[0-9 ,]+\\\\r\", \"\", txt)\n",
    "    txt = re.sub(\"Figure [0-9]+\\..*? [0-9]+.*?Citigroup.*?\\n\", \"\", txt, re.MULTILINE, re.DOTALL)\n",
    "    txt = re.sub(\"Figure [0-9]+\\..*?Source:.*?\\n\", \"\", txt, re.MULTILINE, re.DOTALL)\n",
    "    txt = re.sub(\"Figure [0-9]+\\..*|figure [0-9]+\\..*|Source:.*|source:.*\", \"\", txt)\n",
    "    txt = re.sub(\"[0-9]+, Citi Research\", \"\", txt)\n",
    "    txt = re.sub(\".*[0-9]+.*?\\nCitigroup.*\", \"\", txt)\n",
    "    txt = re.sub(\".*[0-9]+.*?Citigroup.*\", \"\", txt)\n",
    "    txt = re.sub(\".*Citi GPS.*\", \"\", txt)\n",
    "    txt = re.sub(\"Citi|Citibank|Citigroup|citi|citibank|citigroup\", \"\", txt)\n",
    "    txt = re.sub(\"\\n(Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec).*? [0-9]+ *\\r\", \"\", txt)\n",
    "    txt = re.sub(\".*\\?\", \"\", txt)\n",
    "    txt = re.sub(\"!\", \".\", txt)\n",
    "    txt = re.sub(\"\\r|\\n\", \" \", txt)\n",
    "    txt = re.sub(\"[^\\x00-\\x7f]\", \"\", txt)\n",
    "    txt = re.sub(\" +\", \" \", txt)\n",
    "    txt = txt.strip()\n",
    "    \n",
    "    return txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = './Actual - Bank of the Future Text/*.txt' \n",
    "files = glob.glob(path)\n",
    "# iterate over the list getting each file \n",
    "articles_list = []\n",
    "#inside for loop we remove the global redundants\n",
    "for fle in files:\n",
    "    with codecs.open(fle, \"r\",encoding='utf-8', errors='ignore') as f:\n",
    "        article_content = ''\n",
    "        for line in f:\n",
    "            article_content += line\n",
    "        article_content = remove_global_redundancies(article_content)\n",
    "        articles_list.append(article_content)\n",
    "        \n",
    "articles_dict = {'article_text': articles_list}\n",
    "df = pd.DataFrame.from_dict(articles_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function to remove stopwords\n",
    "def remove_stopwords(sen):\n",
    "    sen_new = \" \".join([i for i in sen if i not in STOPWORDS])\n",
    "    return sen_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract word vectors\n",
    "word_embeddings = {}\n",
    "f = open('..\\\\Glove\\\\glove.6B.100d.txt', encoding='utf-8')\n",
    "#f = open('./glove/glove.6B.100d.txt', encoding='utf-8') for Mac\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    word_embeddings[word] = coefs\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get frequency table from text.\n",
    "def get_freq_table(txt):\n",
    "    words = word_tokenize(txt) #list of strings where each string is a word.\n",
    "    freq_table = dict()\n",
    "    \n",
    "    for word in words:\n",
    "        word = STEMMER.stem(word)\n",
    "        if word in STOPWORDS:\n",
    "            continue\n",
    "        if word in freq_table:\n",
    "            freq_table[word] += 1\n",
    "        else:\n",
    "            freq_table[word] = 1\n",
    "       \n",
    "    return freq_table\n",
    "\n",
    "#create giant string which contains all articles (which had global redundancies removed prior)\n",
    "overall_string = \"\"\n",
    "for s in df['article_text']:\n",
    "    overall_string += s + \" \"\n",
    "\n",
    "#global word frequency table\n",
    "FREQ_TABLE = get_freq_table(overall_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pre-cond: txt is a text with global redundancies removed. pkl_dir is the directory of the FREQ_TABLE pickle.\n",
    "\n",
    "def summarize_alg(txt, pkl_dir):\n",
    "    sentences = nltk.sent_tokenize(txt) #list of strings where each string is a sentence\n",
    "    \n",
    "    #-------------clean sentences (non-global redundants)---------------------\n",
    "    \n",
    "    # remove punctuations, numbers and special characters\n",
    "    clean_sentences = pd.Series(sentences).str.replace(\"[^a-zA-Z -]\", \"\").str.replace(\" +\", \" \")\n",
    "    # make alphabets lowercase\n",
    "    clean_sentences = [s.lower() for s in clean_sentences]\n",
    "    # remove stopwords from the sentences\n",
    "    clean_sentences = [remove_stopwords(r.split()) for r in clean_sentences]\n",
    "    \n",
    "    #-------------------------1st algo------------------------\n",
    "    sentence_vectors = []\n",
    "    for i in clean_sentences:\n",
    "        if len(i) != 0:\n",
    "            v = sum([word_embeddings.get(w, np.zeros((100,))) for w in i.split()])/(len(i.split())+0.001)\n",
    "        else:\n",
    "            v = np.zeros((100,))\n",
    "        sentence_vectors.append(v)\n",
    "        \n",
    "    # similarity matrix initialization\n",
    "    sim_mat = np.zeros([len(sentences), len(sentences)])\n",
    "    \n",
    "    # updating matrix coeffs\n",
    "    for i in range(len(sentences)):\n",
    "        for j in range(len(sentences)):\n",
    "            if i != j:\n",
    "                sim_mat[i][j] = cosine_similarity(sentence_vectors[i].reshape(1,100),\n",
    "                                                  sentence_vectors[j].reshape(1,100))[0,0]\n",
    "    \n",
    "    # graphs\n",
    "    nx_graph = nx.from_numpy_matrix(sim_mat)\n",
    "    first_algo_scores = nx.pagerank_numpy(nx_graph, alpha=0.85, personalization=None, weight='weight', dangling=None)\n",
    "    \n",
    "    #normalize across first_algo_scores such that every score runs from [0,1].\n",
    "    max_score_first = max(first_algo_scores.values())\n",
    "    first_algo_scores = {k: ((v/max_score_first) + 1)/2 for k, v in first_algo_scores.items()}\n",
    "    \n",
    "    #--------------------2nd algo-----------------------------------------------\n",
    "    second_algo_scores = dict()\n",
    "    \n",
    "    for i in range(len(clean_sentences)):\n",
    "        sentence = clean_sentences[i]\n",
    "        words_in_sentence = word_tokenize(sentence)\n",
    "        second_algo_scores[i] = 0\n",
    "        for word in words_in_sentence:\n",
    "            if STEMMER.stem(word) in FREQ_TABLE: #we only consider words which appear in FREQ_TABLE.\n",
    "                score_of_word = FREQ_TABLE[STEMMER.stem(word)]\n",
    "                if i in second_algo_scores.keys():\n",
    "                    second_algo_scores[i] += score_of_word\n",
    "                else:\n",
    "                    second_algo_scores[i] = score_of_word\n",
    "        \n",
    "        #divide by length of sentence\n",
    "        if len(sentence)!=0:\n",
    "            second_algo_scores[i]/len(sentence)\n",
    "        \n",
    "    #normalize across second_algo_scores such that every score runs from [0,1].\n",
    "    max_score_second = max(second_algo_scores.values())\n",
    "    second_algo_scores = {k: v/max_score_second for k, v in second_algo_scores.items()}\n",
    "    \n",
    "    #----------------------combine scores----------------------------\n",
    "    combined_scores = {i: (first_algo_scores[i])*(second_algo_scores[i]) for i in range(len(sentences))}\n",
    "    \n",
    "    #----------------------extract the sentences we need-----------------------------------\n",
    "    #list of tuples (index, score) sorted by score\n",
    "    scores_sorted = list(sorted(combined_scores.items(), key=lambda x:-x[1]))\n",
    "    \n",
    "    extracted_sentences = list()\n",
    "    sentence_count = 0\n",
    "    \n",
    "    for (index, score) in scores_sorted:\n",
    "        if len(sentences[index])>=300:\n",
    "            continue\n",
    "        else:\n",
    "            if sentence_count==3:\n",
    "                break\n",
    "            extracted_sentences.append((index, sentences[index]))\n",
    "            sentence_count += 1\n",
    "\n",
    "    #-----------return the sentences in the order they appeared as a single string--------\n",
    "    final_text = functools.reduce(lambda x,y : x+\" \"+y, map(lambda x: x[1], sorted(extracted_sentences)))\n",
    "    \n",
    "    return final_text\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Among other factors, the banks strategic game plan will have to include the greater use of artificial intelligence (AI) and automation and the related overhaul of Core Banking systems and increased adoption of Cloud-based services. Banks are exploring AI uses in consumer and wholesale banking with the help of robotics (automation of routine tasks), analytics (big data mining), chat bots (digital dialogue with customers), and cognitive (changing rules and adapting). Outside the tech sector itself, financial services are one of the leading early adopters of AI in terms of spending, and in our chapter on artificial intelligence, we do a deep dive into some of the use cases of AI in banking and finance today.\n"
     ]
    }
   ],
   "source": [
    "print(summarize_alg(df.iloc[0,0],'FREQ_TABLE.pickle'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Advances in computing power, data volume, and connectivity are core components of the industrialization of AI, and together they are leading an explosion in AI applications, including in financial services. Industrialization of AI Spending and Investing More Banking & Securities Is the Largest Non-Tech Industry for AI Outside the tech sector itself, financial services is one of the leading early adopters of AI. According to the IDC, the banking and securities sector is not only the biggest spender on external AI services but is also expected see fast spending growth over the next five years.\n"
     ]
    }
   ],
   "source": [
    "print(summarize_alg(df.iloc[1,0],'FREQ_TABLE.pickle'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traditional banking is being challenged not by small FinTech startups but rather by established tech giants (particularly in emerging markets) leveraging their strong customer bases, vast user data pools, agile technology platforms, and deep funding pockets. The bank offers no physical branches and operates entirely on a cloud \f",
      " computing platform using Big Data to compute loan amounts and terms, thus saving significantly on operational costs. With 980 million monthly active user accounts as of September 2017 (+16% YoY), Tencents Weixin mobile messaging app has become a powerful multi-function platform, including for integrating third-party services, including payments and financial services.\n"
     ]
    }
   ],
   "source": [
    "print(summarize_alg(df.iloc[2,0],'FREQ_TABLE.pickle'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19.2243390083313"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "end = time.time()\n",
    "end - start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
