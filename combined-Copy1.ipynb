{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "import functools\n",
    "import os\n",
    "import re\n",
    "from gensim.summarization import keywords\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import networkx as nx\n",
    "import codecs\n",
    "import pickle\n",
    "import time\n",
    "\n",
    "#global variable\n",
    "STOPWORDS = set(stopwords.words(\"english\"))\n",
    "STEMMER = SnowballStemmer(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_txt(txtfile):\n",
    "    with open(txtfile, 'r') as myfile:\n",
    "        text=myfile.read()\n",
    "    myfile.close()\n",
    "    return text\n",
    "\n",
    "#pre-con: txt is a raw text with \\n\n",
    "#post-con: returns text with global redundant words removed\n",
    "def remove_global_redundancies(txt):\n",
    "    txt = re.sub(\"\\\\n[0-9 ,]+\\\\r\", \"\", txt)\n",
    "    txt = re.sub(\"Figure [0-9]+\\..*? [0-9]+.*?Citigroup.*?\\n\", \"\", txt, re.MULTILINE, re.DOTALL)\n",
    "    txt = re.sub(\"Figure [0-9]+\\..*?Source:.*?\\n\", \"\", txt, re.MULTILINE, re.DOTALL)\n",
    "    txt = re.sub(\"Figure [0-9]+\\..*|figure [0-9]+\\..*|Source:.*|source:.*\", \"\", txt)\n",
    "    txt = re.sub(\"[0-9]+, Citi Research\", \"\", txt)\n",
    "    txt = re.sub(\".*[0-9]+.*?\\nCitigroup.*\", \"\", txt)\n",
    "    txt = re.sub(\".*[0-9]+.*?Citigroup.*\", \"\", txt)\n",
    "    txt = re.sub(\".*Citi GPS.*\", \"\", txt)\n",
    "    txt = re.sub(\"Citi|Citibank|Citigroup|citi|citibank|citigroup\", \"\", txt)\n",
    "    txt = re.sub(\"\\n(Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec).*? [0-9]+ *\\r\", \"\", txt)\n",
    "    txt = re.sub(\".*\\?\", \"\", txt)\n",
    "    txt = re.sub(\"!\", \".\", txt)\n",
    "    txt = re.sub(\"\\r|\\n\", \" \", txt)\n",
    "    txt = re.sub(\"[^\\x00-\\x7f]\", \"\", txt)\n",
    "    txt = re.sub(\" +\", \" \", txt)\n",
    "    txt = txt.strip()\n",
    "    \n",
    "    return txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = './Actual - Bank of the Future Text/*.txt' \n",
    "files = glob.glob(path)\n",
    "# iterate over the list getting each file \n",
    "articles_list = []\n",
    "#inside for loop we remove the global redundants\n",
    "for fle in files:\n",
    "    with codecs.open(fle, \"r\",encoding='utf-8', errors='ignore') as f:\n",
    "        article_content = ''\n",
    "        for line in f:\n",
    "            article_content += line\n",
    "        article_content = remove_global_redundancies(article_content)\n",
    "        articles_list.append(article_content)\n",
    "        \n",
    "articles_dict = {'article_text': articles_list}\n",
    "df = pd.DataFrame.from_dict(articles_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function to remove stopwords\n",
    "def remove_stopwords(sen):\n",
    "    sen_new = \" \".join([i for i in sen if i not in STOPWORDS])\n",
    "    return sen_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract word vectors\n",
    "word_embeddings = {}\n",
    "f = open('..\\\\Glove\\\\glove.6B.100d.txt', encoding='utf-8')\n",
    "#f = open('./glove/glove.6B.100d.txt', encoding='utf-8') for Mac\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    word_embeddings[word] = coefs\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get frequency table from text.\n",
    "def get_freq_table(txt):\n",
    "    words = word_tokenize(txt) #list of strings where each string is a word.\n",
    "    freq_table = dict()\n",
    "    \n",
    "    for word in words:\n",
    "        word = STEMMER.stem(word)\n",
    "        if word in STOPWORDS:\n",
    "            continue\n",
    "        if word in freq_table:\n",
    "            freq_table[word] += 1\n",
    "        else:\n",
    "            freq_table[word] = 1\n",
    "       \n",
    "    return freq_table\n",
    "\n",
    "#create giant string which contains all articles (which had global redundancies removed prior)\n",
    "overall_string = \"\"\n",
    "for s in df['article_text']:\n",
    "    overall_string += s + \" \"\n",
    "\n",
    "#global word frequency table\n",
    "FREQ_TABLE = get_freq_table(overall_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pre-cond: txt is a text with global redundancies removed. pkl_dir is the directory of the FREQ_TABLE pickle.\n",
    "\n",
    "def summarize_alg(txt, pkl_dir):\n",
    "    sentences = nltk.sent_tokenize(txt) #list of strings where each string is a sentence\n",
    "    \n",
    "    #-------------clean sentences (non-global redundants)---------------------\n",
    "    \n",
    "    # remove punctuations, numbers and special characters\n",
    "    clean_sentences = pd.Series(sentences).str.replace(\"[^a-zA-Z -]\", \"\").str.replace(\" +\", \" \")\n",
    "    # make alphabets lowercase\n",
    "    clean_sentences = [s.lower() for s in clean_sentences]\n",
    "    # remove stopwords from the sentences\n",
    "    clean_sentences = [remove_stopwords(r.split()) for r in clean_sentences]\n",
    "    \n",
    "    #-------------------------1st algo------------------------\n",
    "    sentence_vectors = []\n",
    "    for i in clean_sentences:\n",
    "        if len(i) != 0:\n",
    "            v = sum([word_embeddings.get(w, np.zeros((100,))) for w in i.split()])/(len(i.split())+0.001)\n",
    "        else:\n",
    "            v = np.zeros((100,))\n",
    "        sentence_vectors.append(v)\n",
    "        \n",
    "    # similarity matrix initialization\n",
    "    sim_mat = np.zeros([len(sentences), len(sentences)])\n",
    "    \n",
    "    # updating matrix coeffs\n",
    "    for i in range(len(sentences)):\n",
    "        for j in range(len(sentences)):\n",
    "            if i != j:\n",
    "                sim_mat[i][j] = cosine_similarity(sentence_vectors[i].reshape(1,100),\n",
    "                                                  sentence_vectors[j].reshape(1,100))[0,0]\n",
    "    \n",
    "    # graphs\n",
    "    nx_graph = nx.from_numpy_matrix(sim_mat)\n",
    "    first_algo_scores = nx.pagerank_numpy(nx_graph, alpha=0.85, personalization=None, weight='weight', dangling=None)\n",
    "    \n",
    "    #normalize across first_algo_scores such that every score runs from [0,1].\n",
    "    max_score_first = max(first_algo_scores.values())\n",
    "    first_algo_scores = {k: ((v/max_score_first) + 1)/2 for k, v in first_algo_scores.items()}\n",
    "    \n",
    "    #--------------------2nd algo-----------------------------------------------\n",
    "    second_algo_scores = dict()\n",
    "    \n",
    "    for i in range(len(clean_sentences)):\n",
    "        sentence = clean_sentences[i]\n",
    "        words_in_sentence = word_tokenize(sentence)\n",
    "        second_algo_scores[i] = 0\n",
    "        for word in words_in_sentence:\n",
    "            if STEMMER.stem(word) in FREQ_TABLE: #we only consider words which appear in FREQ_TABLE.\n",
    "                score_of_word = FREQ_TABLE[STEMMER.stem(word)]\n",
    "                if i in second_algo_scores.keys():\n",
    "                    second_algo_scores[i] += score_of_word\n",
    "                else:\n",
    "                    second_algo_scores[i] = score_of_word\n",
    "        \n",
    "        #divide by length of sentence\n",
    "        if len(sentence)!=0:\n",
    "            second_algo_scores[i]/len(sentence)\n",
    "        \n",
    "    #normalize across second_algo_scores such that every score runs from [0,1].\n",
    "    max_score_second = max(second_algo_scores.values())\n",
    "    second_algo_scores = {k: v/max_score_second for k, v in second_algo_scores.items()}\n",
    "    \n",
    "    #----------------------combine scores----------------------------\n",
    "    combined_scores = {i: (first_algo_scores[i])*(second_algo_scores[i]) for i in range(len(sentences))}\n",
    "    \n",
    "    #----------------------extract the sentences we need-----------------------------------\n",
    "    #list of tuples (index, score) sorted by score\n",
    "    scores_sorted = list(sorted(combined_scores.items(), key=lambda x:-x[1]))\n",
    "    \n",
    "    extracted_sentences = list()\n",
    "    sentence_count = 0\n",
    "    \n",
    "    for (index, score) in scores_sorted:\n",
    "        if len(sentences[index])>=300:\n",
    "            continue\n",
    "        else:\n",
    "            if sentence_count==3:\n",
    "                break\n",
    "            extracted_sentences.append((index, sentences[index]))\n",
    "            sentence_count += 1\n",
    "\n",
    "    #-----------return the sentences in the order they appeared as a single string--------\n",
    "    final_text = functools.reduce(lambda x,y : x+\" \"+y, map(lambda x: x[1], sorted(extracted_sentences)))\n",
    "    \n",
    "    return final_text\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.8000550270080566"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start = time.time()\n",
    "summarize_alg(df.iloc[0,0],'FREQ_TABLE.pickle')\n",
    "end = time.time()\n",
    "end - start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2672708034515381"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start = time.time()\n",
    "summarize_alg(df.iloc[1,0],'FREQ_TABLE.pickle')\n",
    "end = time.time()\n",
    "end - start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7418761253356934"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start = time.time()\n",
    "summarize_alg(df.iloc[2,0],'FREQ_TABLE.pickle')\n",
    "end = time.time()\n",
    "end - start"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
